{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MittalNeha/AI-ML_classProjects/blob/master/Github_Diffusion_Training_CLIP_Conditional.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a Text Conditional Image Generation using Diffusion Models:\n",
        "\n",
        "- To see more details on the code and model training visit the [Github Repo](https://github.com/apapiu/guided-diffusion-keras)\n",
        "\n",
        "Make sure to go to Runtime and set your Hardware -> Change Runtime Type and have the accelerator set to GPU!\n",
        "\n",
        "Note that this notebook does **not** read or write anything to your machine nor does it need any access to google drive or outside sources. All the downloads happen within this notebook and once you close it, it will all go gone poof!\n",
        "\n",
        "- Author: Alexandru Papiu\n",
        "- Email: alexpapiu@gmail.com\n"
      ],
      "metadata": {
        "id": "BlE_GFdbbS2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Licensed under the MIT License\n",
        "\n",
        "# Copyright (c) 2022 Alexandru Papiu\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "metadata": {
        "cellView": "form",
        "id": "fYCNVnWkdEFC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yWoJqNX3IJL",
        "outputId": "a82d28ee-0fc3-44ee-8793-cd88bf2d2bd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'guided-diffusion-keras'...\n",
            "remote: Enumerating objects: 121, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "Receiving objects: 100% (121/121), 40.59 KiB | 10.15 MiB/s, done.\n",
            "remote: Total 121 (delta 73), reused 61 (delta 27), pack-reused 0\u001b[K\n",
            "Resolving deltas: 100% (73/73), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ore4zycp\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ore4zycp\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.1+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369370 sha256=6c19b0c56046faf2f5a5e6943b04e3daeb83510e362ff3baa2eba1437e819011\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ptlrgz6u/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n",
            "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
            "Git LFS initialized.\n",
            "Cloning into 'diffusion_model_aesthetic_keras'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Total 34 (delta 0), reused 0 (delta 0), pack-reused 34\u001b[K\n",
            "Unpacking objects: 100% (34/34), 8.04 MiB | 4.29 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "#clone the repo so you can mess around with it:\n",
        "!git clone https://github.com/apapiu/guided-diffusion-keras.git\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "#this is where the data is:\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/apapiu/diffusion_model_aesthetic_keras\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10, cifar100\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "from tensorflow import keras\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "####local imports:\n",
        "sys.path.append(\"/content/guided-diffusion-keras/guided_diffusion\")\n",
        "from denoiser import get_network\n",
        "from utils import batch_generator, plot_images, get_data, preprocess\n",
        "from diffuser import Diffuser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########\n",
        "# CONFIG:\n",
        "#########\n",
        "\n",
        "train_model=True\n",
        "\n",
        "image_size = 64\n",
        "num_channels = 3\n",
        "epochs = 50\n",
        "class_guidance = 4\n",
        "\n",
        "# architecture\n",
        "channels = 96\n",
        "channel_multiplier = [1, 2, 3, 4]\n",
        "block_depth = 2\n",
        "emb_size = 512  # CLIP/label embedding\n",
        "num_classes = 12  # only used if precomputed_embedding=False\n",
        "attention_levels = [0, 0, 1, 0]\n",
        "\n",
        "embedding_dims = 32\n",
        "embedding_max_frequency = 1000.0\n",
        "\n",
        "precomputed_embedding = True\n",
        "save_in_drive = False\n",
        "widths = [c * channels for c in channel_multiplier]\n",
        "\n",
        "###train process config:\n",
        "batch_size = 64\n",
        "num_imgs = 36 #num imgs to test on - should be a square - 25, 64, 100 etc.\n",
        "row = int(np.sqrt(num_imgs))\n",
        "\n",
        "validation_num = 100 #don't train on first validation_num images so we can use as visual validation\n",
        "train_size = 40000 #only train on a subset of train data - do this for faster results..\n",
        "\n",
        "learning_rate = 0.0003\n",
        "\n",
        "MODEL_NAME = \"model_test_aesthetic\"\n",
        "from_scratch = True #if False will load model from model path and continue training\n",
        "file_name = \"from_huggingface\"\n",
        "\n",
        "data_dir = '/content/diffusion_model_aesthetic_keras'\n",
        "captions_path = os.path.join(data_dir, \"captions.csv\")\n",
        "imgs_path = os.path.join(data_dir, \"imgs.npy\")\n",
        "embedding_path = os.path.join(data_dir, \"embeddings.npy\")\n",
        "\n",
        "if save_in_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    drive_path = '/content/drive/MyDrive/'\n",
        "    home_dir = os.path.join(drive_path, MODEL_NAME)\n",
        "else:\n",
        "    home_dir = MODEL_NAME\n",
        "\n",
        "if not os.path.exists(home_dir):\n",
        "    os.mkdir(home_dir)\n",
        "\n",
        "model_path = os.path.join(home_dir, MODEL_NAME + \".h5\")"
      ],
      "metadata": {
        "id": "IvNp0RSNp1op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n06lorn65G_g"
      },
      "outputs": [],
      "source": [
        "##################################\n",
        "###########Loading Data And Model:\n",
        "##################################\n",
        "\n",
        "#load the data from a numpy file file:\n",
        "captions = pd.read_csv(captions_path)\n",
        "train_data, train_label_embeddings = np.load(imgs_path), np.load(embedding_path)\n",
        "#train_data, train_label_embeddings = get_data(npz_file_name=file_name, prop=0.6, captions=False)\n",
        "print(train_data.shape)\n",
        "\n",
        "labels = train_label_embeddings[:num_imgs]\n",
        "\n",
        "np.random.seed(100)\n",
        "rand_image = np.random.normal(0, 1, (num_imgs, image_size, image_size, num_channels))\n",
        "\n",
        "if from_scratch:\n",
        "    autoencoder = get_network(image_size,\n",
        "                              widths,\n",
        "                              block_depth,\n",
        "                              num_classes=num_classes,\n",
        "                              attention_levels=attention_levels,\n",
        "                              emb_size=emb_size,\n",
        "                              num_channels=num_channels,\n",
        "                              precomputed_embedding=precomputed_embedding)\n",
        "\n",
        "    autoencoder.compile(optimizer=\"adam\", loss=\"mae\")\n",
        "else:\n",
        "    autoencoder = keras.models.load_model(model_path)\n",
        "\n",
        "\n",
        "##################\n",
        "#Some data checks:\n",
        "##################\n",
        "\n",
        "print(\"Number of pamaters is {0}\".format(autoencoder.count_params()))\n",
        "pd.Series(train_data[:1000].ravel()).hist(bins=80)\n",
        "plt.show()\n",
        "print(\"Original Validation Images:\")\n",
        "plot_images(preprocess(train_data[:num_imgs]), nrows=int(np.sqrt(num_imgs)))\n",
        "plot_model(autoencoder, to_file=os.path.join(home_dir, 'model_plot.png'),\n",
        "           show_shapes=True, show_layer_names=True)\n",
        "print(\"Validation Captions:\")\n",
        "print(captions[:num_imgs][\"0\"].values)\n",
        "print(\"Generating Images below:\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############################\n",
        "#!create generator and train:\n",
        "#############################\n",
        "\n",
        "diffuser = Diffuser(autoencoder,\n",
        "                    class_guidance=class_guidance,\n",
        "                    diffusion_steps=35)\n",
        "\n",
        "if train_model:\n",
        "    train_generator = batch_generator(autoencoder,\n",
        "                                    model_path,\n",
        "                                    train_data[validation_num:validation_num+train_size],\n",
        "                                    train_label_embeddings[validation_num:validation_num+train_size],\n",
        "                                    epochs,\n",
        "                                    batch_size,\n",
        "                                    rand_image,\n",
        "                                    labels,\n",
        "                                    home_dir,\n",
        "                                    diffuser)\n",
        "\n",
        "    autoencoder.optimizer.learning_rate.assign(learning_rate)\n",
        "\n",
        "    eval_nums = autoencoder.fit(\n",
        "        x=train_generator,\n",
        "        epochs=epochs,\n",
        "    )"
      ],
      "metadata": {
        "id": "shkGkn-AlYbC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}